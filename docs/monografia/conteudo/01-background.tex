%!TeX root=../monografia.tex
%("dica" para o editor de texto: este arquivo é parte de um documento maior)
% para saber mais: https://tex.stackexchange.com/q/78101

\chapter{Background}
\label{cap:background}
% Cap 1 (10 páginas): Background/Revisão bibliográfica. Nessa seção a ideia é contar um pouco do por quê do trabalho, do Isidoro, e das ferramentas (LLM, indexação semântica, Redução de Dimensionalidade, banco de dados vetorial, etc) que estamos usando em um nível mais genérico.

Neste trabalho, utilizaremos ferramentas de modelagem de tópicos e grandes modelos de linguagem com o objetivo de demonstrar sua utilidade no estudo de textos históricos. Nas seções a seguir detalhamos as ferramentas utilizadas e as obras a serem estudadas.

\section{Redes Neurais e Grandes Modelos de Linguagem}

Desde o surgimento dos primeiros computadores digitais no período logo após a Segunda Guerra Mundial (1939 - 1945), pesquisadores trabalharam para desenvolver ferramentas que pudessem executar tarefas que até então eram realizadas apenas por humanos. Cientistas como Warren McCulloch (1898 - 1969) e Walter Pitts (1923 - 1969), um médico e um lógico, estão entre os primeiros a discutirem modelos do cérebro humano, e como estes poderiam ser implementados computacionalmente. Em 1943 publicaram um artigo~\parencite{McCulloch1943} considerado seminal na área, aonde propuseram um modelo matemático do cérebro representado como uma rede de elementos simples interconectados. Um elemento particular recebe sinais dos elementos conectados à sua entrada, e produzem uma soma ponderada como sinal de saída, que, dependendo de um limiar pré-determinado, é enviado para elementos conectados à sua saída como um sinal ativo (valor 1) ou inativo (valor 0). Os autores demonstram no artigo que é possível calcular funções lógicas utilizando diferentes configurações de conexões entre elementos. Posteriormente, estes elementos passaram a ser conhecidos como neurônios artificiais, que serviram de base para o desenvolvimento das redes neurais artificiais, anos mais tarde.

Alan Turing (1912 - 1954), matemático e teórico da computação que ajudou a estabelecer as bases da ciência da computação como disciplina, foi um dos primeiros a se dedicar ao que veio a ser chamado posteriormente de inteligência artificial, de forma mais abrangente. Turing apresenta~\parencite{alan1950a} um conjunto de questões e definições que seriam essenciais para o desenvolvimento da área. Em primeiro lugar, ele pergunta se “máquinas podem pensar”, o que traz o primeiro problema importante, o de falta de definições claras sobre o que é ``pensar'' e, naquele momento, sobre o que seria a ``máquina'' em questão. Como definições do que é pensar, e de forma mais abrangente, do que é inteligência, representam problemas filosóficos importantes e sem resposta objetiva, Turing propôs uma abordagem prática para isso: no lugar de ``máquinas podem pensar'', ele passa a perguntar se ``máquinas podem agir de forma indistinguível de um humano''. Em outras palavras, ele ignora o processo interno pelo qual uma ação é produzida, e se preocupa apenas com o efeito da ação por aqueles que a percebem. Para demonstrar esta ideia, Turing propôs um experimento chamado de Jogo da Imitação. Na versão mais simples do jogo, há três participantes: um humano e um computador que devem responder a perguntas de um juiz humano que só pode fazer perguntas e receber respostas escritas em papel, sem contato direto com os outros dois. Se o juiz, após uma série de perguntas, não for capaz de distinguir o humano da máquina, a máquina venceu o jogo, ou seja, conseguiu se passar por um humano.
Apesar de ser uma definição prática, simples, e que serve aos propósitos do autor, esta ideia de que basta ser percebido como humano para ser considerado inteligente ou pensante é problemática para pensadores de outras áreas, que tratam de aspectos metafísicos da mente e do pensamento. 

Em 1980, o filósofo John Searle (1932 -) propôs o argumento do Quarto Chinês~\parencite{searle1980minds}. Neste experimento, o autor imagina uma pessoa que não fala chinês isolada em um quarto com um livro contendo instruções sobre como interpretar símbolos chineses. Se alguém colocar um texto em chinês por debaixo da porta, por exemplo, a pessoa poderia seguir as instruções do livro para produzir símbolos que representem uma resposta coerente para falantes de chinês. No entanto, esta pessoa estaria apenas seguindo regras sintáticas sem nenhuma compreensão semântica do texto que está produzindo. O autor busca com isso se contrapor a ideias funcionalistas, como a de Turing, de que a mente é apenas um sistema de processamento de informações. Ou seja, para Searle, não basta ser capaz de se comportar como se entendesse uma conversa para uma entidade ser considerada como pensante ou inteligente.

Apesar das críticas, a busca pela inteligência artificial seguiu com maior ou menor intensidade ao longo da segunda metade do século XX. Por exemplo, partindo da ideia de neurônios artificiais de McCulloch e Pitts em 1943, Frank Rosenblatt (1928 - 1971), psicólogo, desenvolveu o Perceptron~\parencite{rosenblatt1958perceptron} em 1957, que é considerada a primeira rede neural artificial. A ideia do Perceptron foi desenvolvida ao longo dos anos, passando por diversos altos e baixos, e com a evolução do hardware existente, cada vez mais capaz, culminou na criação do que passou a ser chamado de Deep Learning~\parencite{alexnet}, por volta de 2012 com o trabalho de Alex Krizhevsky (1986 -), que consiste no uso de grandes redes neurais com milhões e até bilhões de elementos aplicado a problemas de processamento de texto e imagem. Mais recentemente, por volta de 2017, a partir do trabalho de Ashish Vaswani (1986 -) e outros foram criadas novas arquiteturas de conexão entre os neurônios artificiais, batizadas de Transformers~\parencite{vaswani2017attention}, com o objetivo de permitir que mais dados pudessem ser armazenados e processados em conjunto, desta forma possibilitando fornecer mais informações de contexto para um problema computacional de processamento de texto ou imagem. A ideia dos transformers é que possibilitou o surgimento de modelos mais recentes como ChatGPT e Gemini, que são exemplares do que é considerado inteligência artificial hoje em dia. 

Modelos baseados em transformers possuem grande capacidade de mapeamento semântico de dados e de geração de texto verossímil, que podem ser consideradas como suas características mais salientes. Estas capacidades são obtidas com base na observação e processamento de grande quantidade de dados de exemplo, processo que é chamado de ``treinamento'' no jargão da área, realizados com o objetivo de se identificar padrões existentes nos dados para poder determinar, por exemplo, quais palavras que aparecem comumente próximas a outras em certos contextos. Concluída esta etapa de treinamento, o modelo é capaz de produzir texto em resposta a uma questão: primeiro é feito o mapeamento semântico da questão para encontrar termos e sentenças com significado parecido na memória do modelo, e segundo, com base nos termos e sentenças encontrados, o modelo faz a síntese de um texto que tenha verossimilhança. Note o uso do termo verossimilhança no lugar de corretude: como os modelos são criados de acordo com a definição de Turing, basta que forneçam respostas indistinguíveis das de um humano, e cabe a quem coloca a questão avaliar a corretude das respostas fornecidas.

\section{Evolução das redes neurais em PLN}
Hoje, modelos de redes neurais artificiais são prevalentes em diversas áreas de aplicação de machine learning, como classificação e geração de imagens, Processamento de Linguagem Natural (PLN), reconhecimento de fala, etc. A seguir, apresentaremos uma breve (e incompleta) história da evolução desses modelos em algumas sub-áreas de PLN, desde desenvolvimentos anteriores à adoção de redes neurais, até o advento dos Transformers.

\citet{harris54} e \citet{firth57synopsis} argumentam que o sentido de uma palavra pode ser deduzido, em parte, a partir dos contextos onde ela é comumente utilizada - "conhecerás uma palavra pela compania que ela mantém". Essa ideia, que podemos chamar de Hipótese Distribucional, direta ou indiretamente, guiou inúmeros avanços em diferentes campos na área de PLN. \citet{deerwester1990indexing}, com seu modelo de Análise Semântica Latente (LSA), foi um dos pioneiros em operacionalizar essa ideia, obtendo resultados promissores na área de indexação e recuperação de informação. Neste modelo, aplica-se uma técnica de álgebra linear, denominada Decomposição em Valores Singulares (SVD), a uma matriz de frequência (ou alguma outra métrica, como TF-IDF) termo-documento, para se obter uma representação mais densa e de menor dimensionalidade desta matriz. O resultado é uma representação vetorial que consegue capturar relações semânticas entre termos e documentos, mapeando itens com significados similares a lugares comuns em um espaço latente, configurando uma forma primitiva do conceito que hoje conhecemos como \textit{word embeddings}.
\begin{figure}[h]
    \includegraphics[width=.5\textwidth]{lsa}
    \caption{Representação do espaço latente gerado pelo LSA.}
\end{figure}

\citet{bengio2003anp} propôs o uso de redes neurais arificiais para modelar distribuições de probabilidade conjunta de sequências de palavras em um determinado idioma. Na arquitetura proposta, uma rede neural é treinada para executar duas tarefas simultaneamente: Associar cada palavra de um vocabulário a um vetor de \(\mathbb{R}^m\) (\textit{embedding}), e determinar a distribuição de probabilidade condicional de sequências de palavras do vocabulário (isto é, dada uma sequência de palavras, determinar a distribuição de probabilidade da próxima palavra), expressas em termos de suas representações vetoriais. Embora poderosa, essa abordagem ainda tinha uma grande complexidade computacional, inviabilizando o seu uso em grandes conjuntos de dados. Um grande avanço na área, que possibilitou a criação de modelos de embedding com um número bem maior de dimensões, e o uso de volumes muito maiores de dados no treinamento desses modelos, foi a chegada dos modelos \textit{Continuous Skip-gram} e \textit{Continuous Bag-of-Words} (CBOW)~\parencite{mikolov2013word2vec}. Estas arquiteturas dispensaram o uso de camadas ocultas, e tinham como objetivo simplesmente prever uma palavra dadas as palavras ao seu redor (arquitetura CBOW), ou, dada uma palavra, prever as palavras ao seu redor (arquitetura Skip-gram), usando uma janela deslizante de contexto, cujo tamanho é dado como parâmetro da arquitetura. Embora, nessa arquitetura, a ordem das palavras na janela de contexto deixasse de ter importância (no caso do CBOW, usa-se a média dos vetores dessas palavras, por exemplo), os embeddings gerados conseguiam capturar bem melhor o sentido global das palavras, principalmente devido à baixa complexidade computacional do modelo, em relação ao proposto por Bengio, o que possibilitou o uso de um volume muito grande de dados em seu treinamento, e a criação de embeddings de dimensionalidades muito maiores (até 1000 dimensões, comparados com as 50-100 do modelo proposto por Bengio). Porém, vale dizer que este modelo, diferentemente do anterior, não é um modelo de linguagem probabilístico, no sentido de que o seu objetivo não é de modelar distribuições de probabilidades de sequências de palavras, mas de aprender representações semânticas eficientes. Posteriormente, usando técnicas como subamostragem e amostragem negativa, Mikolov mostrou como acelerar o processo de aprendizado e inferência dos modelos, e criar melhores representações~\parencite{mikolov2013dist_representations}.
\begin{figure}[h]
    \includegraphics[width=.5\textwidth]{efficient-models}
    \caption{Arquiteturas CBOW e Skip-gram.}
\end{figure}

Na área de tradução automática de texto, um trabalho pioneiro propôs o uso de uma arquitetura do tipo \textit{encoder-decoder} para traduzir sequências de palavras do Inglês para o Francês~\parencite{sutskever2014seqtoseq}. O era composto por duas redes do tipo \textit{Long Short-Term Memory}, ou LSTM, que é um tipo de rede neural recorrente (RNN) dotada de mecanismos de ``controle de memória'', para lidar com problemas de dependência de longo alcance em sequências. O modelo funcionava em duas etapas: primeiro, o \textit{encoder}, consistindo de uma RNN mapeava sequências de palavras de tamanho variado para um vetor de dimensão fixa, denominado \emph{estado oculto} (também chamado de \emph{vetor de contexto}) iterativamente. Depois, o \textit{decoder} usa o último estado oculto do \textit{encoder} para gerar a sequência de saída. Assim, a predição de cada palavra na saída da rede é influenciada tanto por todas as palavras na entrada, quanto pelas previsões anteriores.

Uma evolução significativa dessa última abordagem foi introduzida no trabalho de \citet{bahdanau2016}, onde foi introduzida uma versão precursora dos mecanismos de atenção modernos. No modelo anterior, sequências de palavras eram mapeadas para um vetor de dimensão fixa, o que fazia com que a performance do modelo deteriorasse rapidamente para sequências longas~\parencite{cho2014properties}. O novo modelo proposto buscava livrar o \textit{encoder} de ter que comprimir todo o contexto em um único vetor de tamanho fixo, gerando uma sequência de estados ocultos que o \textit{decoder} poderia acessar. Além disso, o modelo dispunha de um mecanismo para ``procurar'', entre os estados ocultos, informações relevantes para a predição de uma determinada palavra dentro da sequência, e \emph{selecionar} quais palavras deveriam influenciar a palavra atual. O resultado foi uma melhora significativa na transdução de sequências longas.
\begin{figure}[H]
    \includegraphics[width=.5\textwidth]{rnnsearch}
    \caption{RNNSearch.}
\end{figure}

\section{Transformers e desenvolvimentos subsequentes}
Todos estes desenvolvimentos culminaram na criação da arquitetura batizada de Transformer~\parencite{vaswani2017attention}. Todos os modelos mencionados anteriormente dependem do processamento sequencial de tokens da entrada, devido ao emprego de redes neurais recorrentes. Na arquitetura Transformer, os tokens de entrada são processados em paralelo, o que permitiu uma enorme escalabilidade e aceleração de seus processos de treinamento e inferência. A seguir mostramos uma descrição superficial da sua arquitetura.

\subsection{Arquitetura do modelo Transformer}
Assim como os modelos de transdução já citados \parencite{bengio2003anp, sutskever2014seqtoseq, bahdanau2016}, um Transformer conta com um \textit{encoder} e um \textit{decoder}, porém, com a diferença de não haver RNNs envolvidas.
\begin{figure}[h]
    \includegraphics[width=.5\textwidth]{transformer}
    \caption{Arquitetura de um Transformer.}
\end{figure}

\subsubsection{Encoder}
O encoder começa com uma camada de \textit{embedding}, juntamente com uma camada de codificação posicional, que serve para imbuir os embeddings de informação sobre suas posições, já que não estamos mais utilizando RNNs. Em cima disto, múltiplas camadas idênticas são empilhadas. Cada uma destas camadas é composta por duas sub-camadas. A primeira implementa um mecanismo de atenção paralela (\textit{multi-head attention}), e a segunda é uma simples rede do tipo \textit{multi-layer perceptron} (também chamada de rede \textit{feed-forward}).

\subsubsection{Decoder}
O decoder também é composto por várias camadas idênticas empilhadas, onde cada camada é composta por três subcamadas, duas iguais às subcamadas do encoder, e uma extra, \textit{masked self-attention}, que consiste em uma camada de atenção modificada para evitar que uma palavra gerada pelo modelo ``preste atenção'' em uma palavra subsequente (isto é, seja modificada por ela).

\subsection{Atenção}
Podemos entender os mecanismos de atenção presentes na arquitetura Transformer como uma maneira de palavras ``informarem'' umas às outras quais palavras elas podem influenciar, e de que forma. Mais especificamente, sobre cada embedding (que representa um token, na entrada ou na saída), um bloco de atenção calcula três valores, \(Q\), \(K\) e \(V\) (\textit{query}, \textit{key} e \textit{value}) usando transformações lineares simples. Para cada embedding, o seu valor de \(Q\) é comparado com o valor de \(K\) de si mesmo e todos os outros (ou dos embeddings anteriores, no caso de \textit{masked self-attention}), usando uma função de similaridade (no caso, o produto escalar), seguida de uma normalização (\textit{softmax}). O resultado então é usado para calcular a influência dos embeddings uns sobre os outros (efetivamente, soma-se o valor da atenção para um embedding ao valor do embedding, em conexões residuais):
\begin{equation*}
    \operatorname{Atencao}(Q, K, V) = \operatorname{softmax}\left(\dfrac{QK^T}{\sqrt{d_k}}\right)V
\end{equation*}
onde \(d_k\) é a dimensionalidade de \(Q\) e \(K\).

Após uma camada de atenção há uma rede \textit{feed-forward}, com funções de ativação do tipo ReLU (\textit{Rectified Linear Unit}).
\begin{figure}[H]
    
    \begin{subfigure}[c]{.2\textwidth}
        \includegraphics[width=\textwidth]{attention}
        \caption{Atenção de produto escalar.}
    \end{subfigure}
    \quad \quad \quad
    \begin{subfigure}[c]{.275\textwidth}
        \includegraphics[width=\textwidth]{multihead}
        \caption{Multi-head attention.}
    \end{subfigure}
    
    \caption{Mecanismos de atenção nos Transformers.}
\end{figure}

\subsubsection{\textit{Multi-head attention}}
Ao invés de calcular uma única função de atenção, os valores \(Q\), \(K\) e \(V\) são projetados linearmente \(h\) vezes em espaços de menor dimensionalidade, e vários valores de atenção são calculados em paralelo, e depois esses valores são concatenados e projetados novamente no espaço original, resultando nos valores finais de atenção. Isso permite ao modelo considerar influências entre palavras em diferentes espaços latentes, em diferentes posições.

\subsection{BERT, SBERT e derivados}
A chegada dos Transformers representou um novo paradigma para o uso de redes neurais em PLN, e em outras áreas de aplicação. Pela sua capacidade de modelagem de dependências de longo alcance, e pela sua escalabilidade, modelos baseados em Transformers logo dominaram o cenário de machine learning, impulsionados principalmente por modelos generativos como ChatGPT~\parencite{Radford2018ImprovingLU}, Gemini~\parencite{geminiteam2025}, entre outros. Estes modelos apresentam algumas variações arquitetônicas (como uso apenas de decoders em modelos generativos) mas ainda mantêm a estrutura básica de um Transformer, com seus mecanismos de atenção. Um exemplo de modelo que conta apenas com encoders é o BERT~\parencite{devlin2019bert}. Este modelo é pré-treinado em diferentes tarefas, como predição de tokens ocultos, e predição de próxima sentença, e depois passa por um ajuste fino para tarefas específicas, como detecção de paráfrases, inferência textual, classificação de texto, etc. O seu pré-treinamento faz com que o modelo obtenha um ``entendimento'' básico da linguagem, que melhora consideravelmente o seu desempenho em tarefas específicas posteriores.

Apesar de alcançar bons resultados, o modelo BERT se torna computacionalmente inviável para algumas tarefas, como busca por similaridade semântica em um grande corpus, ou clustering de documentos. Para essas tarefas, é desejável um modelo capaz de gerar embeddings de frases, o que o BERT não foi projetado para fazer. Para mitigar essa limitação surgiu o SBERT~\parencite{reimers2019sbert}, que é uma modificação dos modelos BERT pré-treinados, usando redes neurais siamesas e estruturas de rede do tipo triplet para gerar boas representações semânticas de frases. Essas representações então podem ser comparadas usando distância de cosseno, por exemplo. Os embeddings usados neste trabalho são gerados por descendentes desse modelo.

\section{Modelagem de Tópicos}
TODO: escrever +- 0.5 pagina sobre modelagem de tópicos. É bom ter algumas figuras.

\subsection{BERTopic}
O BERTopic é uma técnica de modelagem de tópicos baseada em modelos de sentence embedding. Mais especificamente, ele usa agrupamentos de representações vetoriais semânticas de trechos de documentos para representar tópicos subjacentes no texto.

Ele é constituído, basicamente, de 4 etapas:
\begin{enumerate}
    \item Extração de text embeddings
    \item Redução de dimensionalidade
    \item Clustering
    \item Extração de representações dos tópicos
\end{enumerate}

Primeiramente, extrai-se representações vetoriais das sentenças do corpus, utilizando modelos de \textit{sentence embedding}. Esses modelos são capazes de capturar o sentido de frases em linguagem natural, projetando-as em um espaço latente. Essas representações, ou \textit{embeddings}, possuem diversas propriedades interessantes e úteis. Uma delas é que textos com significado similar ficam próximos no espaço latente. Podemos então encontrar agrupamentos de frases similares semanticamente nesse espaço latente, e naturalmente, interpretar esses agrupamentos como assuntos presentes no texto.

A etapa de redução de dimensionalidade é importante para o desempenho da etapa posterior do método, de clusterização, por conta de um fenômeno conhecido como a Maldição da Dimensionalidade: em espaços de alta dimensionalidade, os conceitos de proximidade, distância, ou vizinhos mais próximos perdem sua significância~\citep{dimen_hubs}. Este fenômeno então acaba por prejudicar a performance de algoritmos e técnicas que utilizem esses conceitos, como algoritmos de clusterização populares \citep{dimen_hubs, dimen_distance_metrics}.\\
Além disso, nessa etapa também computamos reduções desses embeddings a 2 dimensões, para serem utilizadas posteriormente em visualizações, também utilizando o UMAP. Fora o número de dimensões do espaço resultante, todos os outros parâmetros permanecem os mesmos.

\subsection{UMAP}

\subsection{HDBSCAN}

\subsection{Representação dos tópicos}

\section{Obras e autores estudados}

Neste trabalho utilizaremos edições em inglês das seguintes obras: as Etimologias (\emph{Etymologiae}), de Isidoro de Sevilha, Sobre Agricultura (\emph{De Agri Cultura}), de Catão, e Sobre Ciência Agrícola (\emph{De Re Rustica}), de Varrão. A escolha destas obras não é por acaso: estudiosos da obra de Isidoro consideram que ele se baseou em diversas obras da antiguidade para escrever as Etimologias, particularmente as obras de Catão e Varrão sobre agricultura. Sendo assim, tentaremos encontrar similaridades de ideas entre os textos a partir do uso de modelos computacionais. A seguir apresentaremos detalhes sobre os autores e seu contexto.

Isidoro de Sevilha (c. 560 - 636) foi um clérigo, teólogo e pensador da alta idade média, que é considerado um dos intelectuais mais importantes do seu tempo, e cuja influência foi sentida por muitos séculos após a sua morte. Nasceu em Cartagena, que à época era parte do Reino Visigótico, estado que ocupou as regiões da  Península Ibérica e atual sul da França no período seguinte ao fim do Império Romano do ocidente, e fazia parte de uma família que percentia à elite hispano-romana. Seus três irmãos ocuparam funções importantes na igreja, com destaque para seu irmão mais velho, Leandro de Sevilha (c. 534 - c. 600) foi Bispo de Sevilha, cargo que Isidoro ocuparia após a morte de Leandro. Todos os quatro irmãos são venerados como santos pela Igreja Católica.

Como Bispo de Sevilha, Isidoro exerceu grande influência no seu tempo, presidindo sínodos e concílios importantes, como os de Sevilha e Toledo, protegendo os monastérios, e ainda se envolveu na conversão dos reis Visigodos do Arianismo, uma doutrina cristã não-trinitária, para o cristianismo Calcedoniano, que veio a se tornar a doutrina dominante na Igreja Católica.

Como intelectual, produziu diversas obras, dentre as quais se destacam as Etimologias, que são um conjunto de livros que formam uma enciclopédia etimológica, que resume e organiza o conhecimento de diversos autores da antiguidade clássica. A obra segue a tradição de enciclopedistas clássicos, como Plínio, o Velho (c. 23 - 79), com o uso de ordenação alfabética de tópicos e de uma abordagem literária para o conhecimento, baseada no pensamento analógico.

As Etimologias tratam de temas diversos como gramática, retórica, matemática, direito, a Igreja, heresias, guerra, agricultura, entre outros. A sua influência foi tão grande nos séculos seguintes que algumas das obras clássicas utilizadas como base deixaram de ser lidas e copiadas e acabaram se perdendo no tempo. Era considerado o texto base para a educação sobre o período clássico durante a maior parte da idade média.

O estudo crítico das Etimologias revela suas possíveis fontes clássicas, que na maioria das vezes não são citadas por Isidoro. De acordo com Stephen A. Barney, tradutor para o inglês~\parencite{barney2006etymologies} das Etimologias, é possível identificar que o material dos livros I e II, que tratam de gramática, retórica e dialética (as disciplinas do \emph{trivium}) provavelmente foram extraídos dos Institutos, de Cassiodoro (c. 485 - c. 585), o livro III, sobre matemática (contendo as disciplinas do \emph{quadrivium}), provavelmente foi inspirado em Boécio (c. 480 - 524), e o livro XVII, sobre agricultura, deriva de Catão, o Velho (234 - 149 a.C.) e Varrão (116 - 27 a.C.), para citar alguns exemplos.

Marco Pórcio Catão, o Velho (234 - 149 a.C.), foi um soldado, senador e historiador romano. Nascido em uma família de plebeus, que era a classe baixa de cidadãos livres em Roma, desecndia de gerações de soldados com reputação de bravura, como seu pai e seu bisavô. Ainda na infância, com a morte de seu pai, passou a cuidar das atividades da fazenda família. Como jovem soldado, especula-se que aos 20 anos tenha participado de campanhas das Guerras Púnicas no papel de tribuno militar, uma patente de oficial do exército romano. Ao retornar do campo de batalha para a sua fazenda, e com o apoio do seu vizinho e amigo Lúcio Valério Flaco, iniciou carreira política como \emph{questor}, cargo que possuía diversas atribuições, entre elas a de cobrança de impostos e de supervisão financeira. Daí se seguiram diversos cargos políticos importantes, como \emph{pretor} (magistrado), \emph{consul} (o cargo mais alto da República Romana) e \emph{censor} (magistrado de nível superior). Em paralelo a suas atividades políticas, escreveu diversas obras, cuja maioria infelizmente foi perdida. Escreveu uma história de Roma em sete livros chamada de Origens, uma obra sobre assuntos militares, e a obra sobre agricultura que utilizaremos neste trabalho, a única preservada na íntegra. Além disso, foi famoso orador e teve cerca de 150 discursos registrados.

Marco Terêncio Varrão (116 - 27 a.C.) foi um intelectual e polímata romano, descrito por Petrarca como a ``terceira grande luz'' de Roma, depois de Virgílio e Cícero. Nascido em família pertencente à classe equestre de Roma, abaixo somente da classe senatorial, ocupou cargos políticos ao longo da vida, como \emph{questor}, \emph{pretor} e tribuno do povo. Estudou com o filólogo romano Lúcio Élio Estilo e com o filósofo platonista Antíoco de Ascalão. Foi também um líder militar sem grande prestígio durante a Guerra Civil Cesariana. Foi um escritor prolífico, que produziu cerca de 74 obras sobre temas diversos, entre as quais se destacam a Cronologia Varroniana, que lista as datas de eventos importantes de Roma, e os nove livros das Disciplinas, organizados de acordo com os temas das artes liberais da época, e que serviram de exemplo para enciclopedistas que vieram posteriormente, como Plínio, o Velho, e o próprio Isidoro de Sevilha.

