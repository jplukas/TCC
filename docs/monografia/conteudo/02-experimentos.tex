%!TeX root=../monografia.tex
%("dica" para o editor de texto: este arquivo é parte de um documento maior)
% para saber mais: https://tex.stackexchange.com/q/78101

\chapter{Experimentos}
% Cap 2 (3 páginas): Experimentos. Aqui você pode começar a descrever o setup experimental do trabalho, por exemplo, datasets (os livros), a preparação dos dados (obtenção dos textos, separação em sentenças, etc), a indexação usando bancos de dados vetoriais e embeddings gerados a partir LLMs multi-idiomas, e do processo de geração de tópicos. Depois podemos falar um pouco do design da aplicação que estamos fazendo.

\section{Conjunto de dados}
\subsection{Textos históricos}
O conjunto de dados utilizados neste trabalho consiste nas obras Etimologias - Isidoro de Sevilha, ... TODO: lista de obras

Todos os arquivos estão em formato texto puro, e foram obtidos em... TODO: Como pode-se obter esses arquivos.

\subsection{Preparação dos dados}
Primeiramente, pelo modo como cada livro está formatado, fazemos um processamento inicial para separar cada livro em capítulos, e juntar palavras com hífen. TODO: exemplos.
Então, cada capítulo foi subdividido em sentenças usando a biblioteca Spacy... TODO: expandir. TODO: exemplos

Apesar de algumas tarefas em processamento de linguagem natural serem usadas em outras abordagens de modelagem de tópicos, como remoção de stop-words, lematização, etc, não fazemos esses procedimentos na fase de preparação dos dados para a abordagem utilizada neste trabalho, pois ... TODO: expandir. PERGUNTA: Devo explicar remoção de stop words e lematização?

Depois de um pré-processamento dos textos, terminando em sua divisão em sentenças, geramos embeddings para cada uma dessas sentenças. Os modelos usados para a geração desses embeddings foram: "sentence-transformers/LaBSE", "jinaai/jina-embeddings-v3", "intfloat/multilingual-e5-large-instruct", "nomic-ai/nomic-embed-text-v2-moe".
Esses embeddings então foram armazenados em um banco de dados vetorial, juntamente com cada sentença, e alguns outros metadados, como nome do autor, nome da obra, e uma identificação do capítulo. Esses metadados servem para criar filtros para buscas posteriores nesse banco de dados.

A seguir, fazemos uma redução de dimensionalidade de cada um dos conjuntos de embeddings gerados. Essa etapa é importante para o desempenho da etapa posterior do método, de clusterização, pois... TODO: expandir. Esses embeddings reduzidos são armazenados então, para sua utilização posterior.

\section{Experimentos}
Cada experimento consistiu em aplicar o BERTopic, utilizando os diferentes conjuntos de dados gerados pelos diferentes modelos de embedding, variando o número de tópicos a serem descobertos: 10, 20, 50 e 100 tópicos.