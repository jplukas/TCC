%!TeX root=../monografia.tex
%("dica" para o editor de texto: este arquivo é parte de um documento maior)
% para saber mais: https://tex.stackexchange.com/q/78101

\chapter{Experimentos}
\label{cap:experimentos}
% Cap 2 (3 páginas): Experimentos. Aqui você pode começar a descrever o setup experimental do trabalho, por exemplo, datasets (os livros), a preparação dos dados (obtenção dos textos, separação em sentenças, etc), a indexação usando bancos de dados vetoriais e embeddings gerados a partir LLMs multi-idiomas, e do processo de geração de tópicos. Depois podemos falar um pouco do design da aplicação que estamos fazendo.

Neste capítulo detalhamos os dados utilizados neste trabalho juntamente com o \emph{pipeline} de processamento empregado para a sua coleta e preparação, e a configuração experimental utilizada no estudo.

\section{Conjunto de dados}

Como conjunto de dados a ser explorado neste trabalho, temos as seguintes obras:

\begin{itemize}
\item Etimologias (\emph{Etymologiae})~\parencite{barney2006etymologies}, por Isidoro de Sevilha (c. 560 - 636);
\item Sobre Agricultura (\emph{De Agri Cultura})~\parencite{Cato1934Agriculture}, por Marco Pórcio Catão, o Velho (234 - 149 a.C.);
\item Sobre Ciência Agrícola (\emph{De Re Rustica})~\parencite{Cato1934Agriculture}, por Marco Terêncio Varrão (116 - 27 a.C.).
\end{itemize}

No caso das Etimologias, foi utilizada a tradução~\parencite{barney2006etymologies} do latim para o inglês em formato \emph{pdf}. Para os textos de Catão e Varrão foram utilizadas edições da \emph{Loeb Classical Library}~\parencite{Cato1934Agriculture}, atualmente em domínio público e disponíveis \emph{online}~\parencite{ThayerLacusCurtius} em formato \emph{html}.

\section{Preparação dos dados}
% (TODO: Adicionar figura do esquema geral do processo)

\subsection{Limpeza}

No caso das Etimologias, o texto foi extraído do arquivo \emph{pdf} do livro e convertido para formato de texto puro, removendo-se cabeçalhos, rodapés, números de página, e outros artefatos dessa conversão que não são necessários para a análise. Devido a peculiaridades do formato \emph{pdf} e dificuldades encontradas nas bibliotecas utilizadas para a extração do texto, parte do processo de limpeza teve de ser realizado manualmente. No caso das outras obras, disponíveis em formato \emph{html}, o processamento foi realizado de forma totalmente automática.

\subsection{Pré-processamento}

Primeiramente, pelo modo como cada livro está formatado, fizemos um processamento inicial para separar cada livro em capítulos, e juntar palavras com hífen. Em seguida, cada capítulo foi subdividido em sentenças usando o módulo \emph{SentenceRecognizer} da biblioteca Spacy~\parencite{Honnibal2020Industrialstrength}, que possui diversos recursos para PLN.

Uma particularidade da edição escolhida das Etimologias é que cada capítulo é subdividido em seções relativamente curtas, a maioria tendo somente uma oração. Com isso, podemos fazer a modelagem dos tópicos usando dois níveis diferentes de granularidade, a depender de como definimos um ``documento'', unidade básica de nossa análise:

\begin{itemize}
    \item \textbf{Seções}, já presentes no texto;
    \item \textbf{Sentenças}, delimitadas automaticamente;
\end{itemize}

Em nossa análise, utilizamos as duas abordagens e comparamos os resultados.

\begin{figure}[H]
    \begin{subfigure}[c]{.45\textwidth}
        \includegraphics[width=\textwidth]{sections}
        \caption{Seções.}
    \end{subfigure}
    \begin{subfigure}[c]{.45\textwidth}
        \includegraphics[width=\textwidth]{sentences}
        \caption{Sentenças.}
    \end{subfigure}
    
    \caption{Diferentes formas de separar os textos.}
\end{figure}

\subsection{\textit{Stop words}, \textit{stemming} e lematização}

Tarefas rotineiras de pré-processamento de linguagem natural incluem: remoção de \textit{stop words}, que são palavras muito comuns e irrelevantes, \textit{stemming}, e lematização, que são formas de padronizar palavras, reduzindo-as a sua forma mais básica. Apesar de outras técnicas de modelagem de tópicos adotarem essas tarefas como parte do seu processo de preparação dos dados, não utilizamos essas técnicas para este trabalho, pois os modelos de \textit{embedding} que usamos utilizam informações contextuais de cada palavra em uma sentença, e remover palavras ou modificá-las poderia prejudicar a performance de tais modelos~\parencite{stopwords}.

\subsection{Geração de \textit{embeddings}}

Depois da etapa de pré-processamento dos textos, geramos \textit{embeddings} para cada documento, utilizando modelos do tipo \textit{sentence embedders} pré-treinados. Os modelos usados para a geração desses \textit{embeddings} foram: \textit{sentence-transformers/LaBSE}~\parencite{feng2022labse}, \textit{jinaai/jina-embeddings-v3}~\parencite{jina}, \textit{intfloat/multilingual-e5-large-instruct}~\parencite{wang2024multilingual}, \textit{nomic-ai/nomic-embed-text-v2-moe}~\parencite{nussbaum2025nomic}. Estes modelos foram desenhados e treinados para que sentenças semanticamente similares em línguas diferentes, ou traduções, estejam próximas umas das outras em um espaço latente. Podemos usar estes modelos para comparar textos em diferentes idiomas e analisar suas conexões.

\subsection{Conjuntos de dados}
Após essas etapas iniciais, definimos nossos conjuntos de dados. Cada conjunto é definido por um modelo de embedding e um nível de granularidade, já explicado.
% TODO: exemplos, visualizações dos espaços de embeddings.

\section{Persistência}
Os embeddings gerados, suas reduções, e diversos metadados foram armazenados em um banco de dados do \textit{ChromaDB}, juntamente com cada documento.
% TODO: versão final do modelo de dados no ChromaDB.

Dessa forma, podemos fazer diversos tipos de busca, como busca por similaridade, busca textual, filtrar resultados baseados em metadados de cada documento, como autor, idioma, etc.

\section{Experimentos}
Como já foi explicado, a técnica de modelagem de tópicos usada (BERTopic) é composta de diferentes etapas, com várias possibilidades de escolha de algoritmos para cada etapa, e hiperparâmetros para estes algoritmos. Neste trabalho, foram usados o UMAP para redução de dimensionalidade, e o HDBSCAN para o clustering dos documentos e detecção dos tópicos.

Uma particularidade de interesse do HDBSCAN é que ele pode classificar pontos em um dataset como não pertencentes a nenhum cluster, como ``ruído''. Essa característica é útil para detecção de anomalias em um dataset e para dar uma maior confiabilidade nas classificações dos pontos que pertencem de fato a um cluster, mas pode também ser prejudicial, caso o número outliers detectados seja muito grande. Experimentos preliminares com valores padrão para os hiperparâmetros dos algoritmos mostraram uma forte tendência em classificar pontos como outliers. Com uma grande quantidade de escolhas de hiperparâmetros e modelos de embedding, realizamos uma espécie de busca de grade nesse espaço de parâmetros, para obter melhores resultados.

A seguir apresentamos uma explicação dos parâmetros testados para cada algoritmo.

\subsection{Redução de dimensionalidade}

Para a etapa de redução de dimensionalidade, utilizamos o UMAP. Este algoritmo conta com os seguintes parâmetros:
% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \begin{table}[H]

% \begin{xltabular}{1\textwidth} {
%     |>{\centering\arraybackslash\hsize=0.8\hsize\linewidth=\hsize}X
%     |>{\centering\arraybackslash\hsize=2\hsize\linewidth=\hsize}X
%     |>{\centering\arraybackslash\hsize=0.5\hsize\linewidth=\hsize}X
%     |>{\centering\arraybackslash\hsize=0.7\hsize\linewidth=\hsize}X|
%     }
% \caption{Parâmetros usados para o UMAP} \label{tab:umap_params}\\
% \hline \hline
% \textbf{Nome do parâmetro} & \textbf{Descrição} & \textbf{Valor usado} & \textbf{Nota} \\
% \hline \hline
% \endfirsthead
% \multicolumn{4}{c}%
% {\tablename\ \thetable{} -- continuação} \\
% \hline \hline
% \textbf{Nome do parâmetro} & \textbf{Descrição} & \textbf{Valor usado} & \textbf{Nota} \\
% \hline \hline
% \endhead
% \hline\hline \multicolumn{4}{|r|}{{Continua na próxima página}} \\ \hline \hline
% \endfoot
% \hline
% \endlastfoot
% \hline
% \verb|n_neigbors| & Controla como o UMAP equilibra estrutura local versus global nos dados. Valores maiores levam a uma visão mais global da estrutura dos dados, e valores menores a uma visão mais local. & \verb|10| & O valor padrão para o BERTopic é 15. \\
% \hline
% \verb|n_components| & Dimensionalidade do \textit{dataset} resultante.  & \verb|5| & Valor padrão para o BERTopic. \\
% \hline
% \verb|min_dist| & Controla o quão dispersos os pontos estarão na projeção de baixa dimensionalidade. Valores baixos podem ser interessantes para tarefas de clustering. & \verb|0| & Valor padrão para o BERTopic. \\
% \hline
% \verb|low_memory| & Restringe o uso de memória em detrimento da velocidade da computação. Útil quando há pouca memória disponível. & \verb|false| &  \\
% \hline
% \verb|metric| & Métrica usada para calcular distâncias entre pontos. & \verb|"cosine"| & Valor padrão para o BERTopic. \\
% \hline
% \verb|random_state| & Usado para garantir determinismo. & \verb|42| &  \\
% \hline \hline
% \end{xltabular}


% \end{table}

\begin{table}[ht]
    \centering
    \footnotesize
    \setlength{\tabcolsep}{3pt}
    \renewcommand{\arraystretch}{1.1}
    
    \caption{Parâmetros usados para o UMAP}
    \label{tab:umap_params}
    
    \adjustbox{tabular=|l|l|l|l|, center}{
        \hline
        \textbf{Parâmetro} & \textbf{Descrição} & \textbf{Valores testados} & \textbf{Padrão do BERTopic} \\
        \hline
        \texttt{n\_neighbors} & Controla o equilíbrio entre estrutura local e global. & de \texttt{5} a \texttt{60}, incrementos de \texttt{5} & \texttt{15} \\
        \hline
        \texttt{n\_components} & Dimensionalidade resultante. & de \texttt{5} a \texttt{40}, incrementos de \texttt{5} & \texttt{5}\\
        \hline
        \texttt{min\_dist} & Controla dispersão de pontos. & \texttt{0} & \texttt{0} \\
        \hline
        \texttt{low\_memory} & Menor uso de memória. & \texttt{false} & \texttt{false} \\
        \hline
        \texttt{metric} & Métrica de distância. & \texttt{cosine} & \texttt{cosine} \\
        \hline
        \texttt{random\_state} & Semente determinística. & \texttt{42} & -- \\
        \hline
    }
\end{table}


% TODO: normalmente o BERTopic usa \verb|n_neighbors = 15|, então por que eu mudei esse valor?
A implementação usada foi a da biblioteca umap-learn\footnote{\url{https://umap-learn.readthedocs.io/en/latest/}}.

\subsection{Clustering}

Com os embeddings gerados e suas reduções já computadas, inicializamos o BERTopic de modo a pular essas etapas iniciais e começar pela etapa de clustering. O algoritmo utilizado foi o HDBSCAN, disponível na biblioteca Scikit-learn\footnote{\url{https://scikit-learn.org/stable/modules/generated/sklearn.cluster.HDBSCAN.html}}, e utilizamos os seguintes parâmetros:

% \begin{xltabular}{1\textwidth} {
%     |>{\centering\arraybackslash\hsize=1\hsize\linewidth=\hsize}X
%     |>{\centering\arraybackslash\hsize=1.4\hsize\linewidth=\hsize}X
%     |>{\centering\arraybackslash\hsize=0.3\hsize\linewidth=\hsize}X
%     |>{\centering\arraybackslash\hsize=1.3\hsize\linewidth=\hsize}X|
%     }
% \caption{Parâmetros usados para o HDBSCAN} \label{tab:hdbscan_params}\\
% \hline \hline
% \textbf{Nome do parâmetro} & \textbf{Descrição} & \textbf{Valor usado} & \textbf{Nota} \\
% \hline \hline
% \endfirsthead
% \multicolumn{4}{c}%
% {\tablename\ \thetable{} -- continuação} \\
% \hline \hline
% \textbf{Nome do parâmetro} & \textbf{Descrição} & \textbf{Valor usado} & \textbf{Nota} \\
% \hline \hline
% \endhead
% \hline\hline \multicolumn{4}{|r|}{{Continua na próxima página}} \\ \hline \hline
% \endfoot
% \hline
% \endlastfoot
% \hline
% \verb|min_cluster_size| & Controla o tamanho mínimo e a quantidade de \textit{clusters} gerados. & \verb|10| & Valor padrão para o BERTopic. \\
% \hline
% \verb|min_samples| & Controla a quantidade de pontos classificados como ruído. & \verb|1| & Escolhemos um valor baixo para tentar diminuir a quantidade pontos classificados como outliers. \\
% \hline
% \end{xltabular}

\begin{table}[ht]
    \centering
    \footnotesize
    \setlength{\tabcolsep}{3pt}
    \renewcommand{\arraystretch}{1.1}
    
    \caption{Parâmetros usados para o HDBSCAN}
    \label{tab:hdbscan_params}
    
    \adjustbox{tabular=|l|l|l|l|, center}{
        \hline
        \textbf{Parâmetro} & \textbf{Descrição} & \textbf{Valores testados} & \textbf{Padrão do BERTopic} \\
        \hline
        \texttt{min\_cluster\_size} & Controla tamanho mínimo e qtd. de \textit{clusters}. & de \texttt{10} a \texttt{40}, incrementos de \texttt{5} & \texttt{10} \\
        \hline
        \texttt{min\_samples} & Controla quantidade de pontos de ruído. & de \texttt{5} a \texttt{20}, com incrementos de \texttt{5} & \texttt{10} \\
        \hline
    }
\end{table}

\subsection{Representação}
Como já explicado, dados clusters de documentos como tópicos, o BERTopic calcula o score cTF-IDF para cada palavra em um cluster, elege as palavras mais representativas de um tópico baseado nesses scores e cria uma representação sucinta usando essas palavras.
Para melhorar a representação dos tópicos gerados, fizemos alguns ajustes nestas etapas finais do método:

\subsubsection{Vetorização}
Para calcular o cTF-IDF, primeiramente precisamos calcular a frequência de cada palavra, por documento e por tópico. Para isso usamos a classe \lstinline{CountVectorizer}, do próprio BERTopic. Nessa etapa podemos fazer uso da remoção de stop words, pois os embeddings já foram gerados, para que eles não acabem entrando nas representações dos tópicos. Além disso, usamos também bigramas como termos candidatos para representação.

% \begin{table}[ht]
%     \centering
%     \footnotesize % Fonte ainda menor
%     \setlength{\tabcolsep}{3pt}
%     \renewcommand{\arraystretch}{1.1}
    
%     \caption{Parâmetros usados para o CountVectorizer}
%     \label{tab:countvec_params}
    
%     \begin{tabular}{|l|l|l|l|}
%         \hline
%         \textbf{Parâmetro} & \textbf{Descrição} & \textbf{Valor adotado} & \textbf{Padrão do BERTopic} \\
%         \hline
%         \texttt{min\_cluster\_size} & Controla tamanho mínimo e qtd. de \textit{clusters}. & de \texttt{10} a \texttt{40}, incrementos de \texttt{5} & \texttt{10} \\
%         \hline
%         \texttt{min\_samples} & Controla quantidade de pontos de ruído. & \texttt{1} & \texttt{10} \\
%         \hline
%     \end{tabular}
% \end{table}

\subsubsection{cTF-IDF}
Ao calcular as pontuações cTF-IDF das palavras (e bigramas) no texto, utilizamos duas modificações da métrica original. Fazemos isso passando parâmetros específicos ao instanciar a classe \lstinline{ClassTfidfTransformer}, do BERTopic: \lstinline{ClassTfidfTransformer(True, True)}. Recobrando a fórmula original:
\begin{equation*}
    \operatorname{cTF-IDF}_{t, c} = {f}_{t, c} \cdot \log\left(1 + \dfrac{A}{{f}_t}\right)
\end{equation*}
A versão modificada é dada então pela fórmula:
\begin{equation*}
    \operatorname{cTF-IDF}_{t, c} = \sqrt{{f}_{t, c}} \cdot \log\left(1 + \dfrac{A - {f}_t + 0.5}{{f}_t + 0.5}\right)
\end{equation*}

\subsection{Busca por similaridade semântica}
Uma outra possível aplicação de embeddings semânticos é, dado um conjunto de documentos e um exemplo, encontrar os documentos no conjunto que tenham um conteúdo semântico mais próximo possível do exemplo, baseado nas distâncias entre o embedding do exemplo e cada documento do conjunto: o documento mais parecido com o exemplo será o documento cujo embedding estiver mais próximo da projeção do exemplo no espaço latente. Podemos usar isso para fazer cruzamentos entre diferentes autores, e descobrir citações diretas ou indiretas, por exemplo.

Neste trabalho, também fazemos um mini experimento de similaridade semântica: Buscamos, entre as sentenças presentes em Etimologias, as sentenças mais parecidas com dois trechos do livro Sobre Ciência Agrícola \textit{(De Re Rustica)}, de Varrão (em inglês):

\begin{quote}
    \textbf{varrao\_en.1.48.2.S2}: The beard is called arista from the fact that it is the first part to dry (arescere).
\end{quote}
\begin{quote}
    \textbf{varrao\_en.1.64.1.S0}: Amurca, which is a watery fluid, after it is pressed from the olives is stored along with the dregs in an earthenware vessel.
\end{quote}

Os resultados dos experimentos são apresentados no próximo capítulo.


% Para o estudo de conexões e tópicos existentes na obra, serão utilizadas ferramentas como grandes modelos de linguagem para geração de \emph{embeddings}, como Jina V3~\parencite{jina}, bibliotecas de modelagem de tópicos baseadas em transformers, como BERTopic~\parencite{grootendorst2022bertopic}, e UMAP~\parencite{mcinnes2018umap} para visualização dos \emph{embeddings} em duas dimensões.

% \noindent\textbf{Plano de trabalho e cronograma:}
% As atividades a serem realizadas estão resumidas abaixo. Tanto a lista de atividades quanto o cronograma estão sujeitos a alterações.
% \begin{enumerate}
%     \item Conversas com especialistas para entender questões de interesse
%     \item Estudo das ferramentas a serem utilizadas no projeto
%     \item Preparação do texto para processamento
%     \item Criação de ferramenta para modelagem de tópicos
%     \item Análise dos resultados e validação com especialistas
%     \item Apresentação do trabalho
%     \item Produção do texto final

% \end{enumerate}




%     Estabelecidos os contextos do autores e das obras a serem analisadas, apresentamos a seguir o método proposto para o estudo em questão. Recapitulando o que foi apresentado anteriormente, os modelos de inteligência artificial possuem a capacidade de realizar o mapeamento semântico de textos, o que possibilita, entre outras coisas, a comparação de similaridade entre diferentes trechos com base nos seus significados. Há aqui uma distinção importante a ser feita: esta comparação é baseada na similaridade de conceitos, e não por meio da comparação por igualdade de termos ou frases. Por exemplo, se buscamos pelo termo “judeu” em um texto, apenas por igualdade de termos, provavelmente encontraremos palavras como “judeu” e “judeus”, que possuem os mesmos cinco caracteres iniciais. Por outro lado, se buscarmos por similaridade semântica do termo “judeu”, possivelmente encontraremos “judeu”, “judeus”, “judia”, “judaísmo”, “hebreu”, “hebraico”, entre outros termos relacionados. Esta ideia será explorada detalhadamente mais adiante.
% Sobre o processo de mapeamento semântico, é importante enfatizar que esta interpretação é realizada de forma totalmente automática, sem intervenção humana, e com base apenas em observação de grandes quantidades de texto. Uma característica importante desta abordagem é que ela se baseia puramente na observação da prática de escrita, na forma de textos existentes sobre diversos temas, e não em regras pré-estabelecidas. A simplicidade de automação do processo de “leitura” e análise de milhões de obras rapidamente para a identificação de padrões pode ser considerada como uma grande vantagem desta abordagem. No entanto, como a curadoria destes milhões de textos é praticamente impossível, é quase certo de que a qualidade deste conteúdo será pouco uniforme, com um misto de obras de alta e baixa qualidade. A consequência prática disso é que os resultados obtidos não podem ser utilizados cegamente, mas sim precisam ser interpretados para que sua corretude e utilidade sejam avaliadas no contexto apropriado.
% Uma vez que a fase de análise dos textos esteja concluída, o modelo deve ser capaz de realizar o mapeamento semântico de novos textos não necessariamente observados inicialmente. No jargão de inteligência artificial, esta fase inicial é chamada de treinamento, i.e., o modelo processa grande quantidade de dados para identificar padrões e formar uma base de conhecimento, e a fase seguinte é chamada de inferência, em que o modelo, de fato, infere algo sobre dados fornecidos com base no conhecimento prévio compilado durante o treinamento. No nosso caso, esta inferência implica na projeção do texto fornecido ao modelo em um espaço latente que representa toda a informação semântica capturada durante o treinamento. Na prática isso significa que textos com significado similar são colocados em posições do espaço mais próximas entre si do que textos com significados menos similares. Para demonstrar este conceito, realizamos a indexação do texto completo das Etimologias utilizando um modelo de mapeamento semântico já treinado. Para que esta indexação fosse útil para a nossa finaildade, foi preciso realizar um pré-processamento do texto das obras selecionadas, de modo a criar uma base de dados de sentenças. Por exemplo, se tomarmos as primeiras sentenças do livro I das Etimologias, temos a seguinte estrutura na forma livro.capítulo.tópico.sentença:

% • 1.I.1.S1: "A discipline (disciplina) takes its name from 'learning' (discere), whence it can also be called 'knowledge' (scientia)."
% • 1.I.1.S2: "Now 'know' (scire) is named from 'learn' (discere), because none of us knows unless we have learned."
% • 1.I.1.S3: "A discipline is so named in another way, because 'the full thing is learned' (discitur plena)."
% Desta forma, ao encontrarmos determinada sentença com base em uma busca por similaridade, podemos rapidamente recuperar sua origem e referenciar a seção correspondente do livro, desta forma obtendo seu contexto completo para análise.
